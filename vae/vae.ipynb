{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.layers import *\n",
    "from keras import regularizers\n",
    "from keras.models import Model\n",
    "from keras.datasets import mnist\n",
    "from keras.losses import binary_crossentropy\n",
    "from keras import backend as K\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "import imageio\n",
    "from IPython.display import Image\n",
    "import plot\n",
    "import importlib\n",
    "\n",
    "importlib.reload(plot)\n",
    "\n",
    "import numpy as np"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "slide"
    }
   },
   "source": [
    "## What are autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "* Multilayer artificial neural networks\n",
    "* Uses `representation learning`\n",
    "* Tries to mimic input at the output\n",
    "<img src=\"http://fastforwardlabs.github.io/blog-images/miriam/miriams-figure.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "subslide"
    }
   },
   "source": [
    "### * Map high-dimensional data another space\n",
    "### * Compression\n",
    "### * Learn abstract features in an unsupervised way (labeled data is expensive)\n",
    "### * Denoising and hole-filling\n",
    "<img src=\"hole-fill.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "slideshow": {
     "slide_type": "fragment"
    }
   },
   "source": [
    "## Basic autoencoder example\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "(x_train, _), (x_test, y_test) = mnist.load_data()\n",
    "\n",
    "x_train = x_train.astype('float32') / 255.\n",
    "x_test = x_test.astype('float32') / 255.\n",
    "x_train = x_train.reshape((len(x_train), np.prod(x_train.shape[1:])))\n",
    "x_test = x_test.reshape((len(x_test), np.prod(x_test.shape[1:])))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Build model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras import regularizers\n",
    "\n",
    "encoding_dim = 32\n",
    "input_img = Input(shape=(28 * 28,))\n",
    "encoded = Dense(encoding_dim, activation='relu', activity_regularizer=regularizers.l1(10e-50))(input_img)\n",
    "# encoded = Dense(encoding_dim, activation='relu')(input_img)\n",
    "decoded = Dense(28 * 28, activation='sigmoid')(encoded)\n",
    "autoencoder = Model(input_img, decoded)\n",
    "autoencoder.compile(optimizer='adadelta', loss='binary_crossentropy')\n",
    "\n",
    "# get encoder and decoder separately\n",
    "encoded_input = Input(shape=(encoding_dim, ))\n",
    "decoder_layer = autoencoder.layers[-1]\n",
    "decoder = Model(encoded_input, decoder_layer(encoded_input))\n",
    "encoder = Model(input_img, encoded)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "importlib.reload(plot)\n",
    "autoencoder.fit(x_train, x_train,\n",
    "                epochs=50,\n",
    "                batch_size=256,\n",
    "                shuffle=True,\n",
    "                validation_data=(x_test, x_test),\n",
    "                callbacks=[plot.Plot(x_test, y_test, encoder, decoder, 'AE')])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "IMAGES_PATH = 'images/'\n",
    "GIFS_PATH = 'gifs/'\n",
    "\n",
    "filenames = [IMAGES_PATH + f for f in listdir(IMAGES_PATH) if isfile(join(IMAGES_PATH, f))]\n",
    "reconstruction_images = [imageio.imread(filename) for filename in filenames if 'reconstruction' in filename]\n",
    "pca_ica_images = [imageio.imread(filename) for filename in filenames if 'reconstruction' not in filename]\n",
    "imageio.mimsave(GIFS_PATH + 'reconstruction.gif', reconstruction_images)\n",
    "imageio.mimsave(GIFS_PATH + 'pca_ica.gif', pca_ica_images)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=GIFS_PATH + 'reconstruction.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "Image(filename=GIFS_PATH + 'pca_ica.gif')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "number_of_images = 10\n",
    "encoded_imgs = encoder.predict(x_test[:number_of_images])\n",
    "decoded_imgs = decoder.predict(encoded_imgs)\n",
    "\n",
    "plt.figure(figsize=(20, 4))\n",
    "for i in range(number_of_images):\n",
    "    # display original\n",
    "    ax = plt.subplot(2, number_of_images, i + 1)\n",
    "    plt.imshow(x_test[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "\n",
    "    # display reconstruction\n",
    "    ax = plt.subplot(2, number_of_images, i + 1 + number_of_images)\n",
    "    plt.imshow(decoded_imgs[i].reshape(28, 28))\n",
    "    plt.gray()\n",
    "    ax.get_xaxis().set_visible(False)\n",
    "    ax.get_yaxis().set_visible(False)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Variational Autoencoders"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Probabalistic approach to autoencoders\n",
    "- learns a Latent Variable Model Instead of learning an arbitrary representation\n",
    "- Generally no parameter tuning required\n",
    "\n",
    "<img src=\"https://cdn-images-1.medium.com/max/2000/1*22cSCfmktNIwH5m__u2ffA.png\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Vanilla autoencoder is deterministic\n",
    "- Variational Autoencoder is stochastic. It learns a lattent variable $z$ from inputs $x$\n",
    "\n",
    "> Probabilistic encoder approximating the true posterior $q(z|x)$.\n",
    "***\n",
    "> Generative decoder samples posterior $\\hat{q}(z|x)$ aproximation, which does not rely on any particular input x."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- Learning conditional distributions is facilitated by forcing a prior.\n",
    "$$  z\\sim N(0,I). $$\n",
    "<img src=\"http://fastforwardlabs.github.io/blog-images/miriam/imgs_code/vae.4.png\">\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### A VAE has three basic parts:\n",
    "\n",
    "- 1) An encoder that that learns the parameters (mean and variance) of the underlying latent distribution;\n",
    "- 2) A means of sampling from that distribution;\n",
    "- 3) A decoder that can turn the sample back into an image."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1) Encoder:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "img_size = (28 * 28, )\n",
    "batch_size = 256\n",
    "hidden_size = 128\n",
    "latent_size = 16\n",
    "\n",
    "inputs = Input(shape=img_size, name='encoder_input')\n",
    "x = Dense(hidden_size, activation='relu')(inputs)\n",
    "z_mean = Dense(latent_size, name='z_mean')(x)\n",
    "z_log_var = Dense(latent_size, name='z_log_var')(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2) Sampling function"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sampling(args):\n",
    "    z_mean, z_log_var = args\n",
    "    batch_size = K.shape(z_mean)[0]\n",
    "    latent_size = K.int_shape(z_mean)[1]\n",
    "    epsilon = K.random_normal(shape=(batch_size, latent_size))\n",
    "    return z_mean + K.exp(0.5 * z_log_var) * epsilon\n",
    "\n",
    "z = Lambda(sampling, output_shape=(latent_size,), name='z')([z_mean, z_log_var])\n",
    "\n",
    "encoder = Model(inputs, [z_mean, z_log_var, z], name='encoder')\n",
    "encoder.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3) Decoder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "latent_inputs = Input(shape=(latent_size,), name='z_sampling')\n",
    "x = Dense(hidden_size, activation='relu')(latent_inputs)\n",
    "outputs = Dense(28 * 28, activation='sigmoid')(x)\n",
    "decoder = Model(latent_inputs, outputs, name='decoder')\n",
    "outputs = decoder(encoder(inputs)[2])\n",
    "vae = Model(inputs, outputs, name='vae_mlp')\n",
    "\n",
    "reconstruction_loss = binary_crossentropy(inputs, outputs)\n",
    "reconstruction_loss *= (28 * 28)\n",
    "kl_loss = 1 + z_log_var - K.square(z_mean) - K.exp(z_log_var)\n",
    "kl_loss = K.sum(kl_loss, axis=-1)\n",
    "kl_loss *= -0.5\n",
    "vae_loss = K.mean(reconstruction_loss + kl_loss)\n",
    "vae.add_loss(vae_loss)\n",
    "\n",
    "vae.compile(optimizer='adam')\n",
    "vae.summary()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train network"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "idx = np.arange(5000)\n",
    "np.random.shuffle(idx)\n",
    "\n",
    "vae.fit(x=x_train, y=None,\n",
    "        shuffle=True,\n",
    "        epochs=50,\n",
    "        batch_size=256,\n",
    "        validation_data=(x_test, None))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_valid_noTest = x_test.reshape(-1,28,28,1)\n",
    "# Translate into the latent space\n",
    "encoder = Model(input_img, z_mu)\n",
    "x_valid_noTest_encoded = encoder.predict(X_valid_noTest, batch_size=batch_size)\n",
    "plt.figure(figsize=(10, 10))\n",
    "plt.scatter(x_valid_noTest_encoded[:, 0], x_valid_noTest_encoded[:, 1], c=y_test, cmap='brg')\n",
    "plt.colorbar()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Slideshow",
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
