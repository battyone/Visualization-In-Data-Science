{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "import matplotlib.pyplot as plt\n",
    "from mpl_toolkits import mplot3d\n",
    "import pickle\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open('data.pkl', 'rb') as input:\n",
    "    X_train = pickle.load(input)\n",
    "    char_to_indexes = pickle.load(input)\n",
    "    indexes_to_char = pickle.load(input)  \n",
    "    data_size = pickle.load(input)\n",
    "    vocab_size = pickle.load(input)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1\n",
    "\n",
    "weight_sd = 0.1\n",
    "z_size = hidden_size + vocab_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### activations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigmoid(x):\n",
    "    return 1 / (1 + np.exp(-x))\n",
    "\n",
    "def sigmoid_derivative(y):\n",
    "    return y * (1 - y)\n",
    "    \n",
    "def tanh_derivative(x):\n",
    "    return 1 - np.tanh(x) ** 2\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LSTM"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class LSTM:\n",
    "    def __init__ (self, vocab_size, seq_length, learning_rate):\n",
    "#         x = np.zeros(2 * vocab_size)\n",
    "        self.h = np.zeros(vocab_size)\n",
    "        self.c = np.zeros(vocab_size)\n",
    "        \n",
    "        self.seq_length = seq_length\n",
    "        self.learning_rate = learning_rate\n",
    "        \n",
    "        # state\n",
    "        self.c = np.zeros(vocab_size)\n",
    "        # gates\n",
    "        self.Wf = np.random.random((vocab_size, 2 * vocab_size))\n",
    "        self.Wi = np.random.random((vocab_size, 2 * vocab_size))\n",
    "        self.Wo = np.random.random((vocab_size, 2 * vocab_size))\n",
    "        \n",
    "        # cell state\n",
    "        self.Wc = np.random.random((vocab_size, 2 * vocab_size))\n",
    "        \n",
    "        # gates gradients\n",
    "        self.dWf = np.zeros_like(self.f)\n",
    "        self.dWi = np.zeros_like(self.i)\n",
    "        self.dWo = np.zeros_like(self.o)\n",
    "        \n",
    "        # state gradient\n",
    "        self.dWc = np.zeros_like(self.c)\n",
    "        \n",
    "    def _step_forward(self, h_x):\n",
    "        f = sigmoid(np.dot(self.Wf, h_x))\n",
    "        i = sigmoid(np.dot(self.Wi, h_x))\n",
    "        o = sigmoid(np.dot(self.Wo, h_x))\n",
    "\n",
    "        c_tilde = np.tanh(np.dot(self.Wc, h_x))\n",
    "        \n",
    "        self.c *= f\n",
    "        self.c += i * c_tilde\n",
    "        \n",
    "        h = o * self.tangent(self.c)\n",
    "        return self.c, h, f, i, o, c\n",
    "    \n",
    "   \n",
    "    def _step_backward(self, loss, pcs, f, i, c, o, dfcs, dh, h_x):\n",
    "        loss = np.clip(loss + dh, -6, 6)\n",
    "        #multiply loss by activated cell state to compute output derivative\n",
    "        do = np.tanh(self.c) * loss\n",
    "        #output update = (output deriv * activated output) * input\n",
    "        ou = np.dot(np.atleast_2d(do * self.dtangent(o)).T, np.atleast_2d(h_x))\n",
    "        #derivative of cell state = error * output * deriv of cell state + deriv cell\n",
    "        dcs = np.clip(e * o * self.dtangent(self.c) + dfcs, -6, 6)\n",
    "        #deriv of cell = deriv cell state * input\n",
    "        dc = dcs * i\n",
    "        #cell update = deriv cell * activated cell * input\n",
    "        cu = np.dot(np.atleast_2d(dc * self.dtangent(c)).T, np.atleast_2d(h_x))\n",
    "        #deriv of input = deriv cell state * cell\n",
    "        di = dcs * c\n",
    "        #input update = (deriv input * activated input) * input\n",
    "        iu = np.dot(np.atleast_2d(di * self.dsigmoid(i)).T, np.atleast_2d(h_x))\n",
    "        #deriv forget = deriv cell state * all cell states\n",
    "        df = dcs * pcs\n",
    "        #forget update = (deriv forget * deriv forget) * input\n",
    "        fu = np.dot(np.atleast_2d(df * self.dsigmoid(f)).T, np.atleast_2d(h_x))\n",
    "        #deriv cell state = deriv cell state * forget\n",
    "        dpcs = dcs * f\n",
    "        #deriv hidden state = (deriv cell * cell) * output + deriv output * output * output deriv input * input * output + deriv forget\n",
    "        #* forget * output\n",
    "        dphs = np.dot(dc, self.c)[:self.ys] + np.dot(do, self.o)[:self.ys] + np.dot(di, self.i)[:self.ys] + np.dot(df, self.f)[:self.ys] \n",
    "        #return update gradinets for forget, input, cell, output, cell state, hidden state\n",
    "        return fu, iu, cu, ou, dpcs, dphs\n",
    "            \n",
    "    def update(self, fu, iu, cu, ou):\n",
    "        #update forget, input, cell, and output gradients\n",
    "        self.Gf = 0.9 * self.Gf + 0.1 * fu**2 \n",
    "        self.Gi = 0.9 * self.Gi + 0.1 * iu**2   \n",
    "        self.Gc = 0.9 * self.Gc + 0.1 * cu**2   \n",
    "        self.Go = 0.9 * self.Go + 0.1 * ou**2   \n",
    "        \n",
    "        #update our gates using our gradients\n",
    "        self.f -= self.learning_rate/np.sqrt(self.Gf + 1e-8) * fu\n",
    "        self.i -= self.learning_rate/np.sqrt(self.Gi + 1e-8) * iu\n",
    "        self.c -= self.learning_rate/np.sqrt(self.Gc + 1e-8) * cu\n",
    "        self.o -= self.learning_rate/np.sqrt(self.Go + 1e-8) * ou\n",
    "        return"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
