{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, 'h': 1, 'u': 2, 'V': 3, '_': 4, 'E': 5, 'B': 6, '-': 7, '5': 8, '7': 9, 'm': 10, 'n': 11, 'v': 12, '6': 13, '0': 14, '?': 15, 'G': 16, 'W': 17, 's': 18, 'Q': 19, ']': 20, '”': 21, 'M': 22, 'o': 23, 'f': 24, ';': 25, 'c': 26, 'O': 27, 'k': 28, 'T': 29, \"'\": 30, '*': 31, 'C': 32, '8': 33, 'Z': 34, 'w': 35, 'Y': 36, 'H': 37, '9': 38, 'L': 39, 'x': 40, '@': 41, '/': 42, '3': 43, 'U': 44, 'N': 45, 'j': 46, 'K': 47, 'A': 48, ',': 49, ':': 50, '(': 51, 'l': 52, 'X': 53, 't': 54, 'i': 55, '\\n': 56, '%': 57, 'a': 58, 'D': 59, 'J': 60, '1': 61, 'F': 62, 'R': 63, '\\ufeff': 64, 'z': 65, '.': 66, 'S': 67, '$': 68, '2': 69, 'I': 70, 'y': 71, 'r': 72, 'P': 73, 'e': 74, '“': 75, 'p': 76, '#': 77, ')': 78, 'g': 79, 'q': 80, 'b': 81, 'd': 82, '[': 83, '!': 84, '4': 85}\n"
     ]
    }
   ],
   "source": [
    "data = open('1342-0.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "char_to_indexes = { ch:i for i, ch in enumerate(chars) }\n",
    "indexes_to_char = { i:ch for i, ch in enumerate(chars) }\n",
    "print(char_to_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((data_size, vocab_size))\n",
    "X_train[np.arange(data_size), \n",
    "        [char_to_indexes[char] for char in data]\n",
    "       ] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward step computation for one timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I) $$ \\frac{\\partial (tanh(x))}{\\partial (x)} = 1 - tanh^2(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II) $$ \\frac{\\partial (Ux_t + Ws_{t-1} + b)}{\\partial (Ux_t)} = 1$$\n",
    "$$ \\frac{\\partial (Ux_t + Ws_{t-1} + b)}{\\partial (Ws_{t-1})} = 1$$\n",
    "$$ \\frac{\\partial (Ux_t + Ws_{t-1} + b)}{\\partial (b)} = 1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III.a) $$ \\frac{\\partial (Ws_{t-1})}{\\partial (W)} = s_{t-1}$$\n",
    "$$ \\frac{\\partial (Ws_{t-1})}{\\partial (s_{t-1})} = W$$\n",
    "\n",
    "III.b) $$ \\frac{\\partial (Ux_t)}{\\partial (U)} = x_t$$\n",
    "$$ \\frac{\\partial (Ux_t)}{\\partial (x_t)} = U$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(x, y):\n",
    "    '''\n",
    "    x: input in a one hot encoding\n",
    "    y: index encoding\n",
    "    '''\n",
    "    return -np.log(x[y, 0])\n",
    "\n",
    "def cross_entropy_derivative(y_predicted, y):\n",
    "    '''\n",
    "    y_predicted: input in a one hot encoding\n",
    "    y: index encoding\n",
    "    '''\n",
    "    y_predicted[y] -= 1\n",
    "    return y_predicted\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x)\n",
    "    return exps / np.sum(exps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ss(a):\n",
    "    print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 333,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    def __init__(self, vocab_size,\n",
    "                 loss_function=cross_entropy, loss_function_derivative=cross_entropy_derivative,\n",
    "                 activation_function=np.tanh,\n",
    "                 hidden_size=100, seq_length = 25, learning_rate = 1e-1):\n",
    "        \n",
    "        _variace = 0.01\n",
    "        self.U = np.random.randn(hidden_size, vocab_size) * _variace # to input\n",
    "        self.W = np.random.randn(hidden_size, hidden_size) * _variace # to recurrent\n",
    "        self.V = np.random.randn(vocab_size, hidden_size) * _variace # to output\n",
    "        \n",
    "        self.b_s = np.zeros((hidden_size, 1)) # hidden bias\n",
    "        self.b_y = np.zeros((vocab_size, 1)) # output bias\n",
    "        \n",
    "        self.loss_function = loss_function\n",
    "        self.loss_function_derivative = loss_function_derivative\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.s = {}\n",
    "    \n",
    "    @property\n",
    "    def params(self):\n",
    "        return {'U':self.U, 'W':self.W, 'V':self.V, 'b_s':self.b_y, 'b_y':self.b_y}\n",
    "        \n",
    "    def _step_forward(self, x, prev_s):\n",
    "        next_s = self.activation_function(np.dot(self.U, x.reshape((-1, 1))) +\n",
    "                                          np.dot(self.W, prev_s) + \n",
    "                                          self.b_s)\n",
    "        y_predicted = np.dot(self.V, next_s) + self.b_y\n",
    "        probability_predicted = softmax(y_predicted)\n",
    "        return next_s, y_predicted, probability_predicted\n",
    "    \n",
    "    def forward(self, x, y, s_initial=None):\n",
    "        '''\n",
    "        x: input in a one hot encoding sentence\n",
    "        y: index encoding sentence\n",
    "        '''\n",
    "        y_predicted, probability_predicted = {}, {}\n",
    "        if s_initial is not None:\n",
    "            self.s[-1] = np.copy(s_initial)\n",
    "        else:\n",
    "            self.s[-1] = np.zeros((self.hidden_size, 1))\n",
    "        loss = 0\n",
    "        for t in range(self.seq_length):\n",
    "#             self.s[t] = self.activation_function(np.dot(self.U, x[t]) + np.dot(self.W, s[t - 1]) + self.b_s)\n",
    "#             y_predicted[t] = np.dot(self.V, self.s[t]) + self.b_y\n",
    "#             probability_predicted[t] = softmax(y_predicted[t])\n",
    "            self.s[t], y_predicted[t], probability_predicted[t] = self._step_forward(x[t], self.s[t - 1])\n",
    "            loss += self.loss_function(probability_predicted[t], y[t])\n",
    "        return loss, self.s, y_predicted, probability_predicted \n",
    "    \n",
    "    def backpropagate_loss(self, probability_predicted, x, y):\n",
    "        '''\n",
    "        probability_predicted: it comes from the forward pass\n",
    "        x: input in a one hot encoding sentence\n",
    "        y: index encoding sentence\n",
    "        '''\n",
    "        dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        db_s, db_y = np.zeros_like(self.b_s), np.zeros_like(self.b_y)\n",
    "        ds_previous_time = np.zeros_like(self.s[0])\n",
    "        for t in reversed(range(self.seq_length)):\n",
    "            # propagate gradients\n",
    "            probability_predicted_at_t = np.copy(probability_predicted[t])\n",
    "            d_loss = self.loss_function_derivative(probability_predicted_at_t, y[t])\n",
    "            \n",
    "            # d(loss)/d(V) = (d(sigmoid(Vs))/d(Vs)) * (d(Vs)/dV) = (d(sigmoid(Vs))/d(Vs)) * s\n",
    "            dV += np.dot(d_loss, self.s[t].T)\n",
    "            # d(loss)/d(bias) = (d(sigmoid(Vs))/d(Vs)) * (d(Vs + b)/db) = (d(sigmoid(Vs))/d(Vs)) * 1            \n",
    "            db_y += d_loss\n",
    "            # d(loss)/d(s) = (d(sigmoid(Vs + b))/d(Vs + b)) * (d(Vs + b)/ds) = (d(sigmoid(Vs))/d(Vs)) * V\n",
    "            ds = np.dot(self.V.T, d_loss) + ds_previous_time\n",
    "\n",
    "            # d(s)/d(Ws_t-1 + Ux + b) = d(tanh(Ws_t-1 + Ux + b))/d(Ws_t-1 + Ux + b) = (1 - (tanh^2(Ws_t-1 + Ux + b))) * ds\n",
    "            dsum = (1 - self.s[t] * self.s[t]) * ds\n",
    "            # d(s)/d(Ws_t-1 + Ux + b) = d(tanh(Ws_t-1 + Ux + b))/d(b) = (1 - (tanh^2(Ws_t-1 + Ux + b))) * 1\n",
    "            db_s += dsum\n",
    "            \n",
    "            # d(sum)/d(U) = d(Ws_t-1 + Ux + b)/dU = x.   => dU = d(next_layer)/dU  * d(next_layer_output)            \n",
    "            dU += np.dot(dsum, x[t].reshape((1, -1)))\n",
    "            dW += np.dot(dsum, self.s[t - 1].T)\n",
    "            ds_previous_time = np.dot(self.W.T, dsum) \n",
    "            \n",
    "        return dU, dW, dV, db_s, db_y, self.s[self.seq_length - 1]\n",
    "    \n",
    "    def sample(self, seed, n):\n",
    "        '''\n",
    "        seed: index of first char\n",
    "        n: number of chars to sample\n",
    "        '''\n",
    "        char_one_hot = np.zeros((self.vocab_size, 1))\n",
    "        char_one_hot[seed] = 1\n",
    "        chars = []\n",
    "        state = np.zeros((self.hidden_size, 1))\n",
    "        for t in range(n):\n",
    "            state, y_predicted, probability_predicted = self._step_forward(char_one_hot, state)\n",
    "            index = np.random.choice(range(vocab_size), p=probability_predicted.ravel())\n",
    "            chars.append(ix_to_char[index])\n",
    "            char_one_hot = np.zeros((self.vocab_size, 1))\n",
    "            char_one_hot[index]\n",
    "            \n",
    "        txt = ''.join(chars)\n",
    "        print(txt)\n",
    "        \n",
    "    def set_parameters(self, param_dict):\n",
    "        self.U = param_dict['U']\n",
    "        self.W = param_dict['W']\n",
    "        self.V = param_dict['V']\n",
    "        self.b_s = param_dict['b_s']\n",
    "        self.b_y = param_dict['b_y']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 334,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_generator(data, step=1, chunk_size=10, one_hot_targets=False):\n",
    "    if step > len(data):\n",
    "        raise ValueError\n",
    "    data_pointer = 0\n",
    "    while data_pointer < len(data) - 1:\n",
    "        x_train_batch = data[data_pointer:data_pointer + chunk_size]\n",
    "        y_train_batch = data[data_pointer + 1:data_pointer + chunk_size + 1]\n",
    "        data_pointer += step\n",
    "        if not one_hot_targets:\n",
    "            y_train_batch = np.argmax(y_train_batch, axis=1)\n",
    "        \n",
    "        yield x_train_batch, y_train_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr, params):\n",
    "        self.learning_rate = lr\n",
    "        self.params_to_optimize = {}\n",
    "        self.old_G = {}\n",
    "        for key, param in params.items():\n",
    "            self.params_to_optimize[key] = param\n",
    "            self.old_G[key] = np.zeros_like(param)\n",
    "    \n",
    "    def update(self, grad_params):\n",
    "        if len(grad_params.keys()) is not len(self.params_to_optimize.keys()):\n",
    "            raise ValueError\n",
    "                    \n",
    "        for key, param in grad_params.items():\n",
    "            self.old_G[key] += grad_params[key] * grad_params[key]\n",
    "            self.params_to_optimize[key] -= self.learning_rate * grad_params[key] / np.sqrt(self.old_G[key] + 1e-8)\n",
    "            \n",
    "        return {'U':self.U, 'W':self.W, 'V':self.V, 'b_s':self.b_y, 'b_y':self.b_y}    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 340,
   "metadata": {},
   "outputs": [],
   "source": [
    "adagrad = AdaGrad(learning_rate, rnn.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 344,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[-0.37347306, -0.03630948, -0.22884819, ...,  0.00439498,\n",
       "         0.16571605, -0.00482806],\n",
       "       [ 0.23345268, -0.13469147,  0.02591374, ...,  0.1229603 ,\n",
       "        -0.2651607 ,  0.09500648],\n",
       "       [-0.32134516,  0.27638917, -0.11967573, ...,  0.0209633 ,\n",
       "         0.4332683 , -0.08395563],\n",
       "       ...,\n",
       "       [ 0.1922518 ,  0.20527769,  0.25145856, ...,  0.06774027,\n",
       "         0.34712234, -0.06109274],\n",
       "       [ 0.19275322,  0.05300581,  0.27816178, ...,  0.07317807,\n",
       "        -0.59490114, -0.10103501],\n",
       "       [-0.0593891 , -0.4375691 , -0.00256694, ..., -0.01524624,\n",
       "        -0.00371799,  0.01032936]])"
      ]
     },
     "execution_count": 344,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "rnn.params['U']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 341,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-341-9859ae9a418a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mfor\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtrain_data_generator\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstep\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;36m10\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mchunk_size\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mseq_length\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0mloss\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mhistory_state\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_predicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mprobability_predicted\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mforward\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mdU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0m_\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrnn\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mbackpropagate_loss\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mprobability_predicted\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m         \u001b[0msmooth_loss\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msmooth_loss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.999\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mloss\u001b[0m \u001b[0;34m*\u001b[0m \u001b[0;36m0.001\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-333-bb5f24f48ab9>\u001b[0m in \u001b[0;36mbackpropagate_loss\u001b[0;34m(self, probability_predicted, x, y)\u001b[0m\n\u001b[1;32m     84\u001b[0m             \u001b[0mdU\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreshape\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m-\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     85\u001b[0m             \u001b[0mdW\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdsum\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mt\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 86\u001b[0;31m             \u001b[0mds_previous_time\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdot\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mW\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mT\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdsum\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     87\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     88\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mdU\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdW\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdV\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_s\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdb_y\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0ms\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mseq_length\u001b[0m \u001b[0;34m-\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "smooth_loss = -np.log(1.0 / vocab_size) * seq_length\n",
    "n_epocs = 4\n",
    "for epoc in range(n_epocs):\n",
    "    for x_train, y_train in train_data_generator(X_train, step=10, chunk_size=seq_length):\n",
    "        loss, history_state, y_predicted, probability_predicted = rnn.forward(x_train, y_train)\n",
    "        dU, dW, dV, db_s, db_y, _ = rnn.backpropagate_loss(probability_predicted, x_train, y_train)\n",
    "        rnn.set_parameters(adagrad.update({'U':dU, 'W':dW, 'V':dV, 'b_s':db_y, 'b_y':db_y}))\n",
    "        \n",
    "    n_epoc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 332,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       ...,\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.],\n",
       "       [1., 1., 1., ..., 1., 1., 1.]])"
      ]
     },
     "execution_count": 332,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.ones((100, 1)) * np.ones((1, 80)) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
