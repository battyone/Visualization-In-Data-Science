{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'U': 0, 'G': 1, 'z': 2, 'D': 3, '0': 4, 'o': 5, 'i': 6, '1': 7, '-': 8, 'V': 9, 'X': 10, '(': 11, 's': 12, 'v': 13, 'h': 14, ':': 15, '5': 16, 'B': 17, 'Q': 18, 'I': 19, '8': 20, 'd': 21, 'S': 22, '%': 23, 'O': 24, \"'\": 25, '“': 26, 'b': 27, 'k': 28, 't': 29, 'f': 30, '/': 31, 'r': 32, 'Z': 33, 'u': 34, ';': 35, ' ': 36, '.': 37, '*': 38, 'J': 39, 'x': 40, 'H': 41, 'y': 42, 'g': 43, 'L': 44, 'j': 45, '6': 46, '2': 47, '\\n': 48, '?': 49, 'K': 50, 'e': 51, '#': 52, 'F': 53, '!': 54, ']': 55, 'E': 56, 'w': 57, 'q': 58, 'n': 59, '$': 60, 'c': 61, ')': 62, 'l': 63, 'R': 64, '7': 65, 'p': 66, '_': 67, 'P': 68, 'M': 69, '@': 70, '”': 71, 'a': 72, ',': 73, '9': 74, 'T': 75, '3': 76, 'C': 77, '\\ufeff': 78, 'W': 79, 'm': 80, 'Y': 81, '[': 82, 'N': 83, 'A': 84, '4': 85}\n"
     ]
    }
   ],
   "source": [
    "data = open('1342-0.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "char_to_indexes = { ch:i for i, ch in enumerate(chars) }\n",
    "indexes_to_char = { i:ch for i, ch in enumerate(chars) }\n",
    "print(char_to_indexes)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward step computation for one timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I) $$ \\frac{\\partial (tanh(x))}{\\partial (x)} = 1 - tanh^2(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II) $$ \\frac{\\partial (Ux_t + Ws_{t-1} + b)}{\\partial (Ux_t)} = 1$$\n",
    "$$ \\frac{\\partial (Ux_t + Ws_{t-1} + b)}{\\partial (Ws_{t-1})} = 1$$\n",
    "$$ \\frac{\\partial (Ux_t + Ws_{t-1} + b)}{\\partial (b)} = 1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III.a) $$ \\frac{\\partial (Ws_{t-1})}{\\partial (W)} = s_{t-1}$$\n",
    "$$ \\frac{\\partial (Ws_{t-1})}{\\partial (s_{t-1})} = W$$\n",
    "\n",
    "III.b) $$ \\frac{\\partial (Ux_t)}{\\partial (U)} = x_t$$\n",
    "$$ \\frac{\\partial (Ux_t)}{\\partial (x_t)} = U$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(x, y):\n",
    "    '''\n",
    "    x: input in a one hot encoding\n",
    "    y: index encoding\n",
    "    '''\n",
    "    return -np.log(x[y, 0])\n",
    "\n",
    "def cross_entropy_derivative(y_predicted, y):\n",
    "    '''\n",
    "    y_predicted: input in a one hot encoding\n",
    "    y: index encoding\n",
    "    '''\n",
    "    y_predicted[y] -= 1\n",
    "    return y_predicted\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x)\n",
    "    return exps / np.sum(exps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    def __init__(self, vocab_size,\n",
    "                 loss_function=cross_entropy, loss_function_derivative=cross_entropy_derivative,\n",
    "                 activation_function=np.tanh,\n",
    "                 hidden_size=100, seq_length = 25, learning_rate = 1e-1):\n",
    "        \n",
    "        _variace = 0.01\n",
    "        self.U = np.random.randn(hidden_size, vocab_size) * _variace # to input\n",
    "        self.W = np.random.randn(hidden_size, hidden_size) * _variace # to recurrent\n",
    "        self.V = np.random.randn(vocab_size, hidden_size) * _variace # to output\n",
    "        \n",
    "        self.b_s = np.zeros((hidden_size, 1)) # hidden bias\n",
    "        self.b_y = np.zeros((vocab_size, 1)) # output bias\n",
    "        \n",
    "        self.loss_function = loss_function\n",
    "        self.loss_function_derivative = loss_function_derivative\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.s = {}\n",
    "    \n",
    "    def _step_forward(self, x, prev_s):\n",
    "        next_s = self.activation_function(np.dot(self.U, x) + np.dot(self.W, prev_s) + self.b_s)\n",
    "        y_predicted = np.dot(self.V, next_s) + self.b_y\n",
    "        probability_predicted = softmax(y_predicted)\n",
    "        return next_s, y_predicted, probability_predicted\n",
    "    \n",
    "    def forward(self, x, y, s_initial):\n",
    "        '''\n",
    "        x: input in a one hot encoding sentence\n",
    "        y: index encoding sentence\n",
    "        '''\n",
    "        y_predicted, probability_predicted = {}, {}, {}\n",
    "        self.s[-1] = np.copy(s_initial)\n",
    "        loss = 0\n",
    "        for t in range(self.seq_length):\n",
    "#             self.s[t] = self.activation_function(np.dot(self.U, x[t]) + np.dot(self.W, s[t - 1]) + self.b_s)\n",
    "#             y_predicted[t] = np.dot(self.V, self.s[t]) + self.b_y\n",
    "#             probability_predicted[t] = softmax(y_predicted[t])\n",
    "            self.s[t], y_predicted[t], probability_predicted[t] = self._step_forward(x[t], self.s[t - 1])\n",
    "            loss += loss_function(probability_predicted[t], y[t])\n",
    "        return loss, self.s, y_predicted, probability_predicted \n",
    "    \n",
    "    def backpropagate_loss(self, probability_predicted, x):\n",
    "        dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        db_s, db_y = np.zeros_like(self.b_s), np.zeros_like(self.b_y)\n",
    "        ds_previous_time = np.zeros_like(self.s[0])\n",
    "        for t in reversed(range(self.seq_length)):\n",
    "            # propagate gradients\n",
    "            probability_predicted_at_t = np.copy(probability_predicted[t])\n",
    "            d_loss = loss_function_derivative(probability_predicted_at_t, targets[t])\n",
    "            \n",
    "            # d(loss)/d(V) = (d(sigmoid(Vs))/d(Vs)) * (d(Vs)/dV) = (d(sigmoid(Vs))/d(Vs)) * s\n",
    "            dV += np.dot(d_loss, self.s[t].T)\n",
    "            # d(loss)/d(bias) = (d(sigmoid(Vs))/d(Vs)) * (d(Vs + b)/db) = (d(sigmoid(Vs))/d(Vs)) * 1\n",
    "            db_y += d_loss\n",
    "            # d(loss)/d(s) = (d(sigmoid(Vs + b))/d(Vs + b)) * (d(Vs + b)/ds) = (d(sigmoid(Vs))/d(Vs)) * V\n",
    "            ds = np.dot(self.V.T, d_loss) + ds_previous_time\n",
    "            \n",
    "            # d(s)/d(Ws_t-1 + Ux + b) = d(tanh(Ws_t-1 + Ux + b))/d(Ws_t-1 + Ux + b) = (1 - (tanh^2(Ws_t-1 + Ux + b))) * ds\n",
    "            dsum = (1 - self.s[t] * self.s[t]) * ds\n",
    "            # d(s)/d(Ws_t-1 + Ux + b) = d(tanh(Ws_t-1 + Ux + b))/d(b) = (1 - (tanh^2(Ws_t-1 + Ux + b))) * 1\n",
    "            db_s += dsum\n",
    "            \n",
    "            # d(sum)/d(U) = d(Ws_t-1 + Ux + b)/dU = x.   => dU = d(next_layer)/dU  * d(next_layer_output)\n",
    "            dU += np.dot(dsum, x[t].T)\n",
    "            dW += np.dot(dsum, self.s[t - 1].T)\n",
    "            ds_previous_time = np.dot(self.V.T, dsum) \n",
    "            \n",
    "        return dU, dW, dV, db_s, db_y, self.s[self.seq_length - 1]\n",
    "    \n",
    "    def sample(self, seed, n):\n",
    "        '''\n",
    "        seed: index of first char\n",
    "        n: number of chars to sample\n",
    "        '''\n",
    "        char_one_hot = np.zeros((self.vocab_size, 1))\n",
    "        char_one_hot[seed] = 1\n",
    "        chars = []\n",
    "        state = np.zeros((self.hidden_size, 1))\n",
    "        for t in range(n):\n",
    "            state, y_predicted, probability_predicted = self._step_forward(char_one_hot, state)\n",
    "            index = np.random.choice(range(vocab_size), p=probability_predicted.ravel())\n",
    "            chars.append(ix_to_char[index])\n",
    "            char_one_hot = np.zeros((self.vocab_size, 1))\n",
    "            char_one_hot[index]\n",
    "            \n",
    "        txt = ''.join(chars)\n",
    "        print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 70,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(vocab_size)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bz0q8vzSJ(4)YMWmR]NE 2,P_Yu,4“OJ\n",
      "P4HJb$KyH_z”tHA64ijE#li.jQ”*3B%*g$”“S_)”N6Dt/_J!xq Zish%mSOs;atZy n\n"
     ]
    }
   ],
   "source": [
    "rnn.sample(1, 100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
