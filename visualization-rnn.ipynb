{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'U': 0, 'G': 1, 'z': 2, 'D': 3, '0': 4, 'o': 5, 'i': 6, '1': 7, '-': 8, 'V': 9, 'X': 10, '(': 11, 's': 12, 'v': 13, 'h': 14, ':': 15, '5': 16, 'B': 17, 'Q': 18, 'I': 19, '8': 20, 'd': 21, 'S': 22, '%': 23, 'O': 24, \"'\": 25, '“': 26, 'b': 27, 'k': 28, 't': 29, 'f': 30, '/': 31, 'r': 32, 'Z': 33, 'u': 34, ';': 35, ' ': 36, '.': 37, '*': 38, 'J': 39, 'x': 40, 'H': 41, 'y': 42, 'g': 43, 'L': 44, 'j': 45, '6': 46, '2': 47, '\\n': 48, '?': 49, 'K': 50, 'e': 51, '#': 52, 'F': 53, '!': 54, ']': 55, 'E': 56, 'w': 57, 'q': 58, 'n': 59, '$': 60, 'c': 61, ')': 62, 'l': 63, 'R': 64, '7': 65, 'p': 66, '_': 67, 'P': 68, 'M': 69, '@': 70, '”': 71, 'a': 72, ',': 73, '9': 74, 'T': 75, '3': 76, 'C': 77, '\\ufeff': 78, 'W': 79, 'm': 80, 'Y': 81, '[': 82, 'N': 83, 'A': 84, '4': 85}\n"
     ]
    }
   ],
   "source": [
    "data = open('1342-0.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "char_to_ix = { ch:i for i, ch in enumerate(chars) }\n",
    "ix_to_char = { i:ch for i, ch in enumerate(chars) }\n",
    "print(char_to_ix)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(86, 1)\n"
     ]
    }
   ],
   "source": [
    "vector_for_char_a = np.zeros((vocab_size, 1))\n",
    "vector_for_char_a[char_to_ix['a']] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward step computation for one timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I) $$ \\frac{\\partial (tanh(x))}{\\partial (x)} = 1 - tanh^2(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II) $$ \\frac{\\partial (Ux_t + Ws_{t-1} + b)}{\\partial (Ux_t)} = 1$$\n",
    "$$ \\frac{\\partial (Ux_t + Ws_{t-1} + b)}{\\partial (Ws_{t-1})} = 1$$\n",
    "$$ \\frac{\\partial (Ux_t + Ws_{t-1} + b)}{\\partial (b)} = 1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III.a) $$ \\frac{\\partial (Ws_{t-1})}{\\partial (W)} = s_{t-1}$$\n",
    "$$ \\frac{\\partial (Ws_{t-1})}{\\partial (s_{t-1})} = W$$\n",
    "\n",
    "III.b) $$ \\frac{\\partial (Ux_t)}{\\partial (U)} = x_t$$\n",
    "$$ \\frac{\\partial (Ux_t)}{\\partial (x_t)} = U$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(x, y):\n",
    "    '''\n",
    "    x: input in a one hot encoding\n",
    "    y: index encoding\n",
    "    '''\n",
    "    return -np.log(x[y, 0])\n",
    "\n",
    "def cross_entropy_derivative(y_predicted, y):\n",
    "    '''\n",
    "    y_predicted: input in a one hot encoding\n",
    "    y: index encoding\n",
    "    '''\n",
    "    y_predicted[y] -= 1\n",
    "    return y_predicted\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x)\n",
    "    return exps / np.sum(exps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    def __init__(self, feature_length, \n",
    "                 loss_function, loss_function_derivative, \n",
    "                 activation_function=np.tanh, activation_function_derivative=\n",
    "                 hidden_size=100, seq_length = 25, learning_rate = 1e-1):\n",
    "        \n",
    "        _variace = 0.01\n",
    "        self.U = np.random.randn(hidden_size, vocab_size) * _variace # to input\n",
    "        self.W = np.random.randn(hidden_size, hidden_size) * _variace # to recurrent\n",
    "        self.V = np.random.randn(vocab_size, hidden_size) * _variace # to output\n",
    "        \n",
    "        b_s = np.zeros((hidden_size, 1)) # hidden bias\n",
    "        b_y = np.zeros((vocab_size, 1)) # output bias\n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        self.feature_length = feature_length\n",
    "        \n",
    "        self.s = np.zeros((seq_length, hidden_size))\n",
    "        self.s[-1,:] = np.zeros(hidden_size)\n",
    "    \n",
    "    def _step_forward(self, x, prev_s):\n",
    "        next_s = self.activation_function(np.dot(self.U, x) + np.dot(self.W, prev_s) + self.b_s)\n",
    "        y_predicted = np.dot(self.V, next_s) + self.b_y\n",
    "        probability_predicted = softmax(y_predicted)\n",
    "        return next_s, y_predicted, probability_predicted\n",
    "    \n",
    "    def forward(self, x, y):\n",
    "        '''\n",
    "        x: input in a one hot encoding\n",
    "        y: index encoding\n",
    "        '''\n",
    "        s, y_predicted, probability_predicted = {}, {}, {}\n",
    "        loss = 0\n",
    "        for t in range(self.seq_length):\n",
    "#             self.s[t] = self.activation_function(np.dot(self.U, x[t]) + np.dot(self.W, s[t - 1]) + self.b_s)\n",
    "#             y_predicted[t] = np.dot(self.V, self.s[t]) + self.b_y\n",
    "#             probability_predicted[t] = softmax(y_predicted[t])\n",
    "            self.s[t], y_predicted[t], probability_predicted[t] = self._step_forward(x[t], s[t - 1])\n",
    "            loss += loss_function(probability_predicted[t], y[t])\n",
    "        return loss, s, y_predicted, probability_predicted \n",
    "    \n",
    "    def backpropagate_loss(self, probability_predicted, x):\n",
    "        dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        db_s, db_y = np.zeros_like(self.b_s), np.zeros_like(self.b_y)\n",
    "        ds_previous_time = np.zeros_like(self.s[0])\n",
    "        for t in reversed(range(self.seq_length)):\n",
    "            # propagate gradients\n",
    "            probability_predicted_at_t = np.copy(probability_predicted[t])\n",
    "            d_loss = loss_function_derivative(probability_predicted_at_t, targets[t])\n",
    "            \n",
    "            # d(loss)/d(V) = (d(sigmoid(Vs))/d(Vs)) * (d(Vs)/dV) = (d(sigmoid(Vs))/d(Vs)) * s\n",
    "            dV += np.dot(d_loss, self.s[t].T)\n",
    "            # d(loss)/d(bias) = (d(sigmoid(Vs))/d(Vs)) * (d(Vs + b)/db) = (d(sigmoid(Vs))/d(Vs)) * 1\n",
    "            db_y += d_loss\n",
    "            # d(loss)/d(s) = (d(sigmoid(Vs + b))/d(Vs + b)) * (d(Vs + b)/ds) = (d(sigmoid(Vs))/d(Vs)) * V\n",
    "            ds = np.dot(self.V.T, d_loss) + ds_previous_time\n",
    "            \n",
    "            # d(s)/d(Ws_t-1 + Ux + b) = d(tanh(Ws_t-1 + Ux + b))/d(Ws_t-1 + Ux + b) = (1 - (tanh^2(Ws_t-1 + Ux + b))) * ds\n",
    "            dsum = (1 - self.s[t] * self.s[t]) * ds\n",
    "            # d(s)/d(Ws_t-1 + Ux + b) = d(tanh(Ws_t-1 + Ux + b))/d(b) = (1 - (tanh^2(Ws_t-1 + Ux + b))) * 1\n",
    "            db_s += dsum\n",
    "            \n",
    "            # d(sum)/d(U) = d(Ws_t-1 + Ux + b)/dU = x.   => dU = d(next_layer)/dU  * d(next_layer_output)\n",
    "            dU += np.dot(dsum, x[t].T)\n",
    "            dW += np.dot(dsum, self.s[t - 1].T)\n",
    "            ds_previous_time = np.dot(self.V.T, dsum) \n",
    "            \n",
    "        return dU, dW, dV, db_s, db_y, self.s[self.seq_length - 1]\n",
    "    \n",
    "    def sample(self, seed, n):\n",
    "        first_char = np.zeros((vocab_size, 1))\n",
    "        first_char[seed] = 1\n",
    "        chars = []\n",
    "        for t in range(n):\n",
    "            self._step_forward(first_char, self.s)\n",
    "            h = np.tanh(np.dot(Wxh, x) + np.dot(Whh, h) + bh)\n",
    "            #compute output (unnormalised)\n",
    "            y = np.dot(Why, h) + by\n",
    "            ## probabilities for next chars\n",
    "            p = np.exp(y) / np.sum(np.exp(y))\n",
    "            #pick one with the highest probability \n",
    "            ix = np.random.choice(range(vocab_size), p=p.ravel())\n",
    "            #create a vector\n",
    "            x = np.zeros((vocab_size, 1))\n",
    "            #customize it for the predicted char\n",
    "            x[ix] = 1\n",
    "            #add it to the list\n",
    "            ixes.append(ix)\n",
    "        txt = ''.join(ix_to_char[ix] for ix in ixes)\n",
    "        print '----\\n %s \\n----' % (txt, )\n",
    "        \n",
    "        \n",
    "        \n",
    "        hprev = np.zeros((hidden_size,1)) # reset RNN memory  \n",
    "        #predict the 200 next characters given 'a'\n",
    "        sample(hprev,char_to_ix['a'],200)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
