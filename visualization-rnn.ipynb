{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'p': 0, 'X': 1, 'g': 2, 'V': 3, 'a': 4, 'x': 5, 'W': 6, 'f': 7, '\\ufeff': 8, '*': 9, '4': 10, '“': 11, '#': 12, '1': 13, 'q': 14, 'P': 15, '2': 16, '0': 17, 'M': 18, '?': 19, 'R': 20, 'E': 21, \"'\": 22, 's': 23, 't': 24, 'm': 25, 'd': 26, 'A': 27, ')': 28, '6': 29, 'b': 30, '.': 31, '\\n': 32, '_': 33, 'z': 34, 'S': 35, 'I': 36, ',': 37, ';': 38, '9': 39, '3': 40, 'o': 41, '7': 42, 'T': 43, 'G': 44, 'j': 45, '$': 46, 'U': 47, '@': 48, 'Y': 49, '5': 50, '8': 51, ' ': 52, 'Q': 53, 'n': 54, '/': 55, 'h': 56, 'K': 57, 'J': 58, 'N': 59, 'C': 60, 'i': 61, 'l': 62, '[': 63, 'F': 64, 'k': 65, '-': 66, ']': 67, 'e': 68, 'H': 69, 'L': 70, 'u': 71, '!': 72, '%': 73, '”': 74, 'w': 75, ':': 76, 'Z': 77, '(': 78, 'r': 79, 'D': 80, 'O': 81, 'v': 82, 'B': 83, 'y': 84, 'c': 85}\n"
     ]
    }
   ],
   "source": [
    "data = open('1342-0.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "char_to_indexes = { ch:i for i, ch in enumerate(chars) }\n",
    "indexes_to_char = { i:ch for i, ch in enumerate(chars) }\n",
    "print(char_to_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((data_size, vocab_size))\n",
    "X_train[np.arange(data_size), \n",
    "        [char_to_indexes[char] for char in data]\n",
    "       ] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "seq_length = 50\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward step computation for one timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I) $$ \\frac{\\partial (tanh(x))}{\\partial (x)} = 1 - tanh^2(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II) $$ \\frac{\\partial (Ux_t + Ws_{t-1} + b)}{\\partial (Ux_t)} = 1$$\n",
    "$$ \\frac{\\partial (Ux_t + Ws_{t-1} + b)}{\\partial (Ws_{t-1})} = 1$$\n",
    "$$ \\frac{\\partial (Ux_t + Ws_{t-1} + b)}{\\partial (b)} = 1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III.a) $$ \\frac{\\partial (Ws_{t-1})}{\\partial (W)} = s_{t-1}$$\n",
    "$$ \\frac{\\partial (Ws_{t-1})}{\\partial (s_{t-1})} = W$$\n",
    "\n",
    "III.b) $$ \\frac{\\partial (Ux_t)}{\\partial (U)} = x_t$$\n",
    "$$ \\frac{\\partial (Ux_t)}{\\partial (x_t)} = U$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(x, y):\n",
    "    '''\n",
    "    x: input in a one hot encoding\n",
    "    y: index encoding\n",
    "    '''\n",
    "    return -np.log(x[y, 0])\n",
    "\n",
    "def cross_entropy_derivative(y_predicted, y):\n",
    "    '''\n",
    "    y_predicted: input in a one hot encoding\n",
    "    y: index encoding\n",
    "    '''\n",
    "    y_predicted[y] -= 1\n",
    "    return y_predicted\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x)\n",
    "    return exps / np.sum(exps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ss(a):\n",
    "    print(a.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    def __init__(self, vocab_size,\n",
    "                 loss_function=cross_entropy, loss_function_derivative=cross_entropy_derivative,\n",
    "                 activation_function=np.tanh,\n",
    "                 hidden_size=100, seq_length = 25, learning_rate = 1e-1):\n",
    "        \n",
    "        _variace = 0.01\n",
    "        self.U = np.random.randn(hidden_size, vocab_size) * _variace # to input\n",
    "        self.W = np.random.randn(hidden_size, hidden_size) * _variace # to recurrent\n",
    "        self.V = np.random.randn(vocab_size, hidden_size) * _variace # to output\n",
    "        \n",
    "        self.b_s = np.zeros((hidden_size, 1)) # hidden bias\n",
    "        self.b_y = np.zeros((vocab_size, 1)) # output bias\n",
    "        \n",
    "        self.loss_function = loss_function\n",
    "        self.loss_function_derivative = loss_function_derivative\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.s = {}\n",
    "    \n",
    "    @property\n",
    "    def params(self):\n",
    "        return {'U':self.U, 'W':self.W, 'V':self.V, 'b_s':self.b_s, 'b_y':self.b_y}\n",
    "        \n",
    "    def _step_forward(self, x, prev_s):\n",
    "        next_s = self.activation_function(np.dot(self.U, x.reshape((-1, 1))) +\n",
    "                                          np.dot(self.W, prev_s) + \n",
    "                                          self.b_s)\n",
    "        y_predicted = np.dot(self.V, next_s) + self.b_y\n",
    "        probability_predicted = softmax(y_predicted)\n",
    "        return next_s, y_predicted, probability_predicted\n",
    "    \n",
    "    def forward(self, x, y, s_initial=None):\n",
    "        '''\n",
    "        x: input in a one hot encoding sentence\n",
    "        y: index encoding sentence\n",
    "        '''\n",
    "        y_predicted, probability_predicted = {}, {}\n",
    "        if s_initial is not None:\n",
    "            self.s[-1] = np.copy(s_initial)\n",
    "        else:\n",
    "            self.s[-1] = np.zeros((self.hidden_size, 1))\n",
    "        loss = 0\n",
    "        for t in range(self.seq_length):\n",
    "#             self.s[t] = self.activation_function(np.dot(self.U, x[t]) + np.dot(self.W, s[t - 1]) + self.b_s)\n",
    "#             y_predicted[t] = np.dot(self.V, self.s[t]) + self.b_y\n",
    "#             probability_predicted[t] = softmax(y_predicted[t])\n",
    "            self.s[t], y_predicted[t], probability_predicted[t] = self._step_forward(x[t], self.s[t - 1])\n",
    "            loss += self.loss_function(probability_predicted[t], y[t])\n",
    "        return loss, self.s, y_predicted, probability_predicted \n",
    "    \n",
    "    def backpropagate_loss(self, probability_predicted, x, y):\n",
    "        '''\n",
    "        probability_predicted: it comes from the forward pass\n",
    "        x: input in a one hot encoding sentence\n",
    "        y: index encoding sentence\n",
    "        '''\n",
    "        dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        db_s, db_y = np.zeros_like(self.b_s), np.zeros_like(self.b_y)\n",
    "        ds_previous_time = np.zeros_like(self.s[0])\n",
    "        for t in reversed(range(self.seq_length)):\n",
    "            # propagate gradients\n",
    "            probability_predicted_at_t = np.copy(probability_predicted[t])\n",
    "            d_loss = self.loss_function_derivative(probability_predicted_at_t, y[t])\n",
    "            \n",
    "            # d(loss)/d(V) = (d(sigmoid(Vs))/d(Vs)) * (d(Vs)/dV) = (d(sigmoid(Vs))/d(Vs)) * s\n",
    "            dV += np.dot(d_loss, self.s[t].T)\n",
    "            # d(loss)/d(bias) = (d(sigmoid(Vs))/d(Vs)) * (d(Vs + b)/db) = (d(sigmoid(Vs))/d(Vs)) * 1            \n",
    "            db_y += d_loss\n",
    "            # d(loss)/d(s) = (d(sigmoid(Vs + b))/d(Vs + b)) * (d(Vs + b)/ds) = (d(sigmoid(Vs))/d(Vs)) * V\n",
    "            ds = np.dot(self.V.T, d_loss) + ds_previous_time\n",
    "\n",
    "            # d(s)/d(Ws_t-1 + Ux + b) = d(tanh(Ws_t-1 + Ux + b))/d(Ws_t-1 + Ux + b) = (1 - (tanh^2(Ws_t-1 + Ux + b))) * ds\n",
    "            dsum = (1 - self.s[t] * self.s[t]) * ds\n",
    "            # d(s)/d(Ws_t-1 + Ux + b) = d(tanh(Ws_t-1 + Ux + b))/d(b) = (1 - (tanh^2(Ws_t-1 + Ux + b))) * 1\n",
    "            db_s += dsum\n",
    "            \n",
    "            # d(sum)/d(U) = d(Ws_t-1 + Ux + b)/dU = x.   => dU = d(next_layer)/dU  * d(next_layer_output)            \n",
    "            dU += np.dot(dsum, x[t].reshape((1, -1)))\n",
    "            dW += np.dot(dsum, self.s[t - 1].T)\n",
    "            ds_previous_time = np.dot(self.W.T, dsum) \n",
    "            \n",
    "        return dU, dW, dV, db_s, db_y, self.s[self.seq_length - 1]\n",
    "    \n",
    "    def sample(self, seed, n):\n",
    "        '''\n",
    "        seed: index of first char\n",
    "        n: number of chars to sample\n",
    "        '''\n",
    "        char_one_hot = np.zeros((self.vocab_size, 1))\n",
    "        char_one_hot[seed] = 1\n",
    "        chars = []\n",
    "        state = np.zeros((self.hidden_size, 1))\n",
    "        for t in range(n):\n",
    "            state, y_predicted, probability_predicted = self._step_forward(char_one_hot, state)\n",
    "            index = np.random.choice(range(vocab_size), p=probability_predicted.ravel())\n",
    "            chars.append(ix_to_char[index])\n",
    "            char_one_hot = np.zeros((self.vocab_size, 1))\n",
    "            char_one_hot[index]\n",
    "            \n",
    "        txt = ''.join(chars)\n",
    "        print(txt)\n",
    "        \n",
    "    def set_parameters(self, param_dict):\n",
    "        self.U = param_dict['U']\n",
    "        self.W = param_dict['W']\n",
    "        self.V = param_dict['V']\n",
    "        self.b_s = param_dict['b_s']\n",
    "        self.b_y = param_dict['b_y']\n",
    "        "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train generator\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_generator(data, step=1, chunk_size=10, one_hot_targets=False):\n",
    "    if step > len(data):\n",
    "        raise ValueError\n",
    "    data_pointer = 0\n",
    "    while data_pointer < len(data) - 1:\n",
    "        x_train_batch = data[data_pointer:data_pointer + chunk_size]\n",
    "        y_train_batch = data[data_pointer + 1:data_pointer + chunk_size + 1]\n",
    "        data_pointer += step\n",
    "        if not one_hot_targets:\n",
    "            y_train_batch = np.argmax(y_train_batch, axis=1)\n",
    "        \n",
    "        yield x_train_batch, y_train_batch"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Optimizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad:\n",
    "    def __init__(self, lr, params):\n",
    "        self.learning_rate = lr\n",
    "        self.params_to_optimize = {}\n",
    "        self.old_G = {}\n",
    "        for key, param in params.items():\n",
    "            self.params_to_optimize[key] = param\n",
    "            self.old_G[key] = np.zeros_like(param)\n",
    "    \n",
    "    def update(self, grad_params):\n",
    "        if len(grad_params.keys()) is not len(self.params_to_optimize.keys()):\n",
    "            raise ValueError\n",
    "                    \n",
    "        for key, param in grad_params.items():\n",
    "            self.old_G[key] += grad_params[key] * grad_params[key]\n",
    "            self.params_to_optimize[key] -= self.learning_rate * grad_params[key] / np.sqrt(self.old_G[key] + 1e-8)\n",
    "            \n",
    "        return self.params_to_optimize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "adagrad = AdaGrad(learning_rate, rnn.params)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/4 [00:00<?, ?it/s]\n",
      "0it [00:00, ?it/s]\u001b[A\n",
      "7it [00:00, 64.89it/s]\u001b[A\n",
      "13it [00:00, 61.10it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "111.35558429203955\n",
      "97.46677578109335\n",
      "103.34693629613913\n",
      "138.3635108993314\n",
      "170.8392208481192\n",
      "125.00747806624832\n",
      "105.62230322906257\n",
      "90.82702344460576\n",
      "111.56204100399418\n",
      "103.30750974183073\n",
      "103.3530068286688\n",
      "105.42317666688953\n",
      "108.60682086529677\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "22it [00:00, 70.12it/s]\u001b[A\n",
      "29it [00:00, 64.42it/s]\u001b[A"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "99.12115407991689\n",
      "90.999788611963\n",
      "100.47515121120566\n",
      "82.89884860058028\n",
      "76.80001893913047\n",
      "80.40067597822159\n",
      "67.27248587138945\n",
      "65.7208827818976\n",
      "64.1797912685276\n",
      "64.7109933282169\n",
      "61.25535442586281\n",
      "62.62125788195251\n",
      "61.11323395356342\n",
      "78.66025607546987\n",
      "60.79857205686266\n",
      "77.31205917092164\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "34it [00:00, 59.98it/s]\u001b[A"
     ]
    }
   ],
   "source": [
    "smooth_loss = -np.log(1.0 / vocab_size) * seq_length\n",
    "n_epocs = 4\n",
    "for epoc in tqdm(range(n_epocs)):\n",
    "    for x_train, y_train in tqdm(train_data_generator(X_train, step=1, chunk_size=seq_length)):\n",
    "        loss, history_state, y_predicted, probability_predicted = rnn.forward(x_train, y_train)\n",
    "        dU, dW, dV, db_s, db_y, _ = rnn.backpropagate_loss(probability_predicted, x_train, y_train)\n",
    "        params = adagrad.update({'U':dU, 'W':dW, 'V':dV, 'b_s':db_s, 'b_y':db_y})\n",
    "        rnn.set_parameters(params)\n",
    "        print(loss)\n",
    "    n_epoc += 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
