{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data load"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{' ': 0, 'h': 1, 'u': 2, 'V': 3, '_': 4, 'E': 5, 'B': 6, '-': 7, '5': 8, '7': 9, 'm': 10, 'n': 11, 'v': 12, '6': 13, '0': 14, '?': 15, 'G': 16, 'W': 17, 's': 18, 'Q': 19, ']': 20, '”': 21, 'M': 22, 'o': 23, 'f': 24, ';': 25, 'c': 26, 'O': 27, 'k': 28, 'T': 29, \"'\": 30, '*': 31, 'C': 32, '8': 33, 'Z': 34, 'w': 35, 'Y': 36, 'H': 37, '9': 38, 'L': 39, 'x': 40, '@': 41, '/': 42, '3': 43, 'U': 44, 'N': 45, 'j': 46, 'K': 47, 'A': 48, ',': 49, ':': 50, '(': 51, 'l': 52, 'X': 53, 't': 54, 'i': 55, '\\n': 56, '%': 57, 'a': 58, 'D': 59, 'J': 60, '1': 61, 'F': 62, 'R': 63, '\\ufeff': 64, 'z': 65, '.': 66, 'S': 67, '$': 68, '2': 69, 'I': 70, 'y': 71, 'r': 72, 'P': 73, 'e': 74, '“': 75, 'p': 76, '#': 77, ')': 78, 'g': 79, 'q': 80, 'b': 81, 'd': 82, '[': 83, '!': 84, '4': 85}\n"
     ]
    }
   ],
   "source": [
    "data = open('1342-0.txt', 'r').read()\n",
    "chars = list(set(data))\n",
    "data_size, vocab_size = len(data), len(chars)\n",
    "char_to_indexes = { ch:i for i, ch in enumerate(chars) }\n",
    "indexes_to_char = { i:ch for i, ch in enumerate(chars) }\n",
    "print(char_to_indexes)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train = np.zeros((data_size, vocab_size))\n",
    "X_train[np.arange(data_size), \n",
    "        [char_to_indexes[char] for char in data]\n",
    "       ] = 1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### hyperparameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "hidden_size = 100\n",
    "seq_length = 25\n",
    "learning_rate = 1e-1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RNN"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Backward step computation for one timestep"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I) $$ \\frac{\\partial (tanh(x))}{\\partial (x)} = 1 - tanh^2(x)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "II) $$ \\frac{\\partial (Ux_t + Ws_{t-1} + b)}{\\partial (Ux_t)} = 1$$\n",
    "$$ \\frac{\\partial (Ux_t + Ws_{t-1} + b)}{\\partial (Ws_{t-1})} = 1$$\n",
    "$$ \\frac{\\partial (Ux_t + Ws_{t-1} + b)}{\\partial (b)} = 1$$\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "III.a) $$ \\frac{\\partial (Ws_{t-1})}{\\partial (W)} = s_{t-1}$$\n",
    "$$ \\frac{\\partial (Ws_{t-1})}{\\partial (s_{t-1})} = W$$\n",
    "\n",
    "III.b) $$ \\frac{\\partial (Ux_t)}{\\partial (U)} = x_t$$\n",
    "$$ \\frac{\\partial (Ux_t)}{\\partial (x_t)} = U$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def cross_entropy(x, y):\n",
    "    '''\n",
    "    x: input in a one hot encoding\n",
    "    y: index encoding\n",
    "    '''\n",
    "    return -np.log(x[y, 0])\n",
    "\n",
    "def cross_entropy_derivative(y_predicted, y):\n",
    "    '''\n",
    "    y_predicted: input in a one hot encoding\n",
    "    y: index encoding\n",
    "    '''\n",
    "    y_predicted[y] -= 1\n",
    "    return y_predicted\n",
    "\n",
    "def softmax(x):\n",
    "    exps = np.exp(x)\n",
    "    return exps / np.sum(exps)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "class RNN():\n",
    "    def __init__(self, vocab_size,\n",
    "                 loss_function=cross_entropy, loss_function_derivative=cross_entropy_derivative,\n",
    "                 activation_function=np.tanh,\n",
    "                 hidden_size=100, seq_length = 25, learning_rate = 1e-1):\n",
    "        \n",
    "        _variace = 0.01\n",
    "        self.U = np.random.randn(hidden_size, vocab_size) * _variace # to input\n",
    "        self.W = np.random.randn(hidden_size, hidden_size) * _variace # to recurrent\n",
    "        self.V = np.random.randn(vocab_size, hidden_size) * _variace # to output\n",
    "        \n",
    "        self.b_s = np.zeros((hidden_size, 1)) # hidden bias\n",
    "        self.b_y = np.zeros((vocab_size, 1)) # output bias\n",
    "        \n",
    "        self.loss_function = loss_function\n",
    "        self.loss_function_derivative = loss_function_derivative\n",
    "        self.activation_function = activation_function\n",
    "        \n",
    "        \n",
    "        self.learning_rate = learning_rate\n",
    "        self.hidden_size = hidden_size\n",
    "        self.seq_length = seq_length\n",
    "        self.vocab_size = vocab_size\n",
    "        self.s = {}\n",
    "    \n",
    "    def _step_forward(self, x, prev_s):\n",
    "        next_s = self.activation_function(np.dot(self.U, x) + np.dot(self.W, prev_s) + self.b_s)\n",
    "        y_predicted = np.dot(self.V, next_s) + self.b_y\n",
    "        probability_predicted = softmax(y_predicted)\n",
    "        return next_s, y_predicted, probability_predicted\n",
    "    \n",
    "    def forward(self, x, y, s_initial):\n",
    "        '''\n",
    "        x: input in a one hot encoding sentence\n",
    "        y: index encoding sentence\n",
    "        '''\n",
    "        y_predicted, probability_predicted = {}, {}, {}\n",
    "        self.s[-1] = np.copy(s_initial)\n",
    "        loss = 0\n",
    "        for t in range(self.seq_length):\n",
    "#             self.s[t] = self.activation_function(np.dot(self.U, x[t]) + np.dot(self.W, s[t - 1]) + self.b_s)\n",
    "#             y_predicted[t] = np.dot(self.V, self.s[t]) + self.b_y\n",
    "#             probability_predicted[t] = softmax(y_predicted[t])\n",
    "            self.s[t], y_predicted[t], probability_predicted[t] = self._step_forward(x[t], self.s[t - 1])\n",
    "            loss += loss_function(probability_predicted[t], y[t])\n",
    "        return loss, self.s, y_predicted, probability_predicted \n",
    "    \n",
    "    def backpropagate_loss(self, probability_predicted, x):\n",
    "        dU, dW, dV = np.zeros_like(self.U), np.zeros_like(self.W), np.zeros_like(self.V)\n",
    "        db_s, db_y = np.zeros_like(self.b_s), np.zeros_like(self.b_y)\n",
    "        ds_previous_time = np.zeros_like(self.s[0])\n",
    "        for t in reversed(range(self.seq_length)):\n",
    "            # propagate gradients\n",
    "            probability_predicted_at_t = np.copy(probability_predicted[t])\n",
    "            d_loss = loss_function_derivative(probability_predicted_at_t, targets[t])\n",
    "            \n",
    "            # d(loss)/d(V) = (d(sigmoid(Vs))/d(Vs)) * (d(Vs)/dV) = (d(sigmoid(Vs))/d(Vs)) * s\n",
    "            dV += np.dot(d_loss, self.s[t].T)\n",
    "            # d(loss)/d(bias) = (d(sigmoid(Vs))/d(Vs)) * (d(Vs + b)/db) = (d(sigmoid(Vs))/d(Vs)) * 1\n",
    "            db_y += d_loss\n",
    "            # d(loss)/d(s) = (d(sigmoid(Vs + b))/d(Vs + b)) * (d(Vs + b)/ds) = (d(sigmoid(Vs))/d(Vs)) * V\n",
    "            ds = np.dot(self.V.T, d_loss) + ds_previous_time\n",
    "            \n",
    "            # d(s)/d(Ws_t-1 + Ux + b) = d(tanh(Ws_t-1 + Ux + b))/d(Ws_t-1 + Ux + b) = (1 - (tanh^2(Ws_t-1 + Ux + b))) * ds\n",
    "            dsum = (1 - self.s[t] * self.s[t]) * ds\n",
    "            # d(s)/d(Ws_t-1 + Ux + b) = d(tanh(Ws_t-1 + Ux + b))/d(b) = (1 - (tanh^2(Ws_t-1 + Ux + b))) * 1\n",
    "            db_s += dsum\n",
    "            \n",
    "            # d(sum)/d(U) = d(Ws_t-1 + Ux + b)/dU = x.   => dU = d(next_layer)/dU  * d(next_layer_output)\n",
    "            dU += np.dot(dsum, x[t].T)\n",
    "            dW += np.dot(dsum, self.s[t - 1].T)\n",
    "            ds_previous_time = np.dot(self.V.T, dsum) \n",
    "            \n",
    "        return dU, dW, dV, db_s, db_y, self.s[self.seq_length - 1]\n",
    "    \n",
    "    def sample(self, seed, n):\n",
    "        '''\n",
    "        seed: index of first char\n",
    "        n: number of chars to sample\n",
    "        '''\n",
    "        char_one_hot = np.zeros((self.vocab_size, 1))\n",
    "        char_one_hot[seed] = 1\n",
    "        chars = []\n",
    "        state = np.zeros((self.hidden_size, 1))\n",
    "        for t in range(n):\n",
    "            state, y_predicted, probability_predicted = self._step_forward(char_one_hot, state)\n",
    "            index = np.random.choice(range(vocab_size), p=probability_predicted.ravel())\n",
    "            chars.append(ix_to_char[index])\n",
    "            char_one_hot = np.zeros((self.vocab_size, 1))\n",
    "            char_one_hot[index]\n",
    "            \n",
    "        txt = ''.join(chars)\n",
    "        print(txt)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "rnn = RNN(vocab_size)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Train"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_data_generator(data, step=1, chunk_size=10, one_hot_targets=False):\n",
    "    if step > len(data):\n",
    "        raise ValueError\n",
    "    data_pointer = 0\n",
    "    while data_pointer < len(data) - 1:\n",
    "        x_train_batch = data[data_pointer:data_pointer + chunk_size]\n",
    "        y_train_batch = data[data_pointer + 1:data_pointer + chunk_size + 1]\n",
    "        data_pointer += step\n",
    "        if not one_hot_targets:\n",
    "            y_train_batch = np.argmax(y_train_batch, axis=1)\n",
    "        \n",
    "        yield x_train_batch, y_train_batch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [],
   "source": [
    "class AdaGrad():\n",
    "    def __init__(self, lr, **params):\n",
    "        self.learning_rate = lr\n",
    "        self.params_to_optimize = {}\n",
    "        self.old_G = {}\n",
    "        for key, param in params.iteritems():\n",
    "            self.params_to_optimize[key] = param\n",
    "            self.old_G[key] = np.zeros_like(param)\n",
    "    \n",
    "    def update(self, **grad_params):\n",
    "        if len(grad_params.keys()) is not len(self.params_to_optimize.keys()):\n",
    "            raise ValueError\n",
    "                    \n",
    "        for key, param in grad_params.iteritems():\n",
    "            self.G[key] += grad_params[key] * grad_params[key]\n",
    "            self.params_to_optimize[key] -= self.learning_rate * grad_params[key] / np.sqrt(self.G[key] + 1e-8)\n",
    "            "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mWxh, mWhh, mWhy = np.zeros_like(Wxh), np.zeros_like(Whh), np.zeros_like(Why)\n",
    "mbh, mby = np.zeros_like(bh), np.zeros_like(by) # memory variables for Adagrad\n",
    "smooth_loss = -np.log(1.0 / vocab_size) * seq_length\n",
    "n_epoc = 0\n",
    "\n",
    "while n_epoc < n_epocs:\n",
    "    for x_train, y_train in train_data_generator(X_train, step=10, chunk_size=seq_length):\n",
    "        loss, history_state, y_predicted, probability_predicted = rnn.forward(x_train, y_train)\n",
    "        dU, dW, dV, db_s, db_y, _ = rnn.backpropagate_loss(probability_predicted, x_train)\n",
    "        smooth_loss = smooth_loss * 0.999 + loss * 0.001\n",
    "\n",
    "        # perform parameter update with Adagrad                                                                                                                                                     \n",
    "        for param, dparam, mem in zip([Wxh, Whh, Why, bh, by],\n",
    "                                    [dWxh, dWhh, dWhy, dbh, dby],\n",
    "                                    [mWxh, mWhh, mWhy, mbh, mby]):\n",
    "            mem += dparam * dparam\n",
    "            param += -learning_rate * dparam / np.sqrt(mem + 1e-8) # adagrad update                                                                                                                   \n",
    "\n",
    "    n_epoc += 1"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
